{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SaveCheckpoints_cifar_wideresnet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2cb6b90b477b438ba2af846e0cabab0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_76792b37395c4136ac3b7ce5a55298d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b5485aec2d834159822729e410c13bc9","IPY_MODEL_21631d2547c14da9a81ebdd668d3530f"]}},"76792b37395c4136ac3b7ce5a55298d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5485aec2d834159822729e410c13bc9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17703acd868f402f9c42b7183eabb27c","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb1174692fff400ca259932e8de6ab80"}},"21631d2547c14da9a81ebdd668d3530f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a4e51d076c38451e9013bab165b82492","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:09&lt;00:00,  9.29s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3df760f7e978479f85b8373e491d461b"}},"17703acd868f402f9c42b7183eabb27c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eb1174692fff400ca259932e8de6ab80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a4e51d076c38451e9013bab165b82492":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3df760f7e978479f85b8373e491d461b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b6c892e174142698f9bf6a0a120f3b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4c9c2fbd77c3432282a8b2299662fcff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1dab66868254c42acc64128809f2bac","IPY_MODEL_d72c7e2fbf9644eda4fb558486d245ee"]}},"4c9c2fbd77c3432282a8b2299662fcff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1dab66868254c42acc64128809f2bac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ba303995f9d841e9ada3d1c409bb3c58","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8dba1c4bfca34a95b5cc43bec59f6fc7"}},"d72c7e2fbf9644eda4fb558486d245ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85569d53987547b8a379035877742146","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 162/162 [00:09&lt;00:00, 17.53 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5574b1dddd22410eae0534436f0e49c7"}},"ba303995f9d841e9ada3d1c409bb3c58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8dba1c4bfca34a95b5cc43bec59f6fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85569d53987547b8a379035877742146":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5574b1dddd22410eae0534436f0e49c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62e5245d8d9c455bb48b006d0433a377":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1b9b940ac62441edb9200dca3e1b2def","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_be6c6d2d8a144c768670a998e3e44f49","IPY_MODEL_47be908210af4b04ab73ec7a66118a89"]}},"1b9b940ac62441edb9200dca3e1b2def":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be6c6d2d8a144c768670a998e3e44f49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b98391be273e4680a729a52f7a9a052d","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12bd1070db1e429cac60031aba072d40"}},"47be908210af4b04ab73ec7a66118a89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_061337a0e2a34af4a7bc2aea00e7d52d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:09&lt;00:00,  9.18s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd7bb6165bfa4122ba0758befcbd1cff"}},"b98391be273e4680a729a52f7a9a052d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"12bd1070db1e429cac60031aba072d40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"061337a0e2a34af4a7bc2aea00e7d52d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bd7bb6165bfa4122ba0758befcbd1cff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0934e9a8c1444c6baeb3e1748c968b8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd5527beca1a4e58bff2a18202171f16","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_62853fb5c3e142f8832ee280b58f474e","IPY_MODEL_023c8d471485427ca561c9f3250575cd"]}},"bd5527beca1a4e58bff2a18202171f16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62853fb5c3e142f8832ee280b58f474e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5e0a1070e90e4d1a93d5dd22702d21ad","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ca8eccaacac458ba5a617c6606d988e"}},"023c8d471485427ca561c9f3250575cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a727af553c94444aa786a198f738947","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 50000/0 [00:37&lt;00:00, 1367.02 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f52a1311f11f4512885776216f30f4dc"}},"5e0a1070e90e4d1a93d5dd22702d21ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2ca8eccaacac458ba5a617c6606d988e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a727af553c94444aa786a198f738947":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f52a1311f11f4512885776216f30f4dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9cf79ac5d390414a84d1160a09f67d86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ec656655591544d5998327bf54096e7d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ea393c44c4eb44fe806aa96393a4dac5","IPY_MODEL_b5f99d7cb7014d31902597db4cf3a305"]}},"ec656655591544d5998327bf54096e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea393c44c4eb44fe806aa96393a4dac5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ef3f8567cf8c4dbe8913b7be22b313c3","_dom_classes":[],"description":" 82%","_model_name":"FloatProgressModel","bar_style":"danger","max":50000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40966,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5dfad64b7e214402ab296cb552a6b12d"}},"b5f99d7cb7014d31902597db4cf3a305":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b00b17562f1e4387ae0f3e9adad170e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40966/50000 [00:03&lt;00:00, 37931.93 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_240370c6ee464cfdb0cc87322a96e012"}},"ef3f8567cf8c4dbe8913b7be22b313c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5dfad64b7e214402ab296cb552a6b12d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b00b17562f1e4387ae0f3e9adad170e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"240370c6ee464cfdb0cc87322a96e012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a4910d538f247fc8830804295c17e30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b406825416034f37aa4f235092bfb629","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_40f08bf714e647569de204c1c5d5579d","IPY_MODEL_35143605a3f24bdba281b24ee2fbcc25"]}},"b406825416034f37aa4f235092bfb629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40f08bf714e647569de204c1c5d5579d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4e3b92c013fc488eafb639e72e99b3ef","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_099ed082d9d8402b95e52c723370bf2a"}},"35143605a3f24bdba281b24ee2fbcc25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7328765b790940b5bab55bdf5704a9b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/0 [00:07&lt;00:00, 1356.80 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67a37925c5694f208e82f56cb8818a36"}},"4e3b92c013fc488eafb639e72e99b3ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"099ed082d9d8402b95e52c723370bf2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7328765b790940b5bab55bdf5704a9b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"67a37925c5694f208e82f56cb8818a36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"55c24605a1194bb2b35e9866c8d27b85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3a58d58dc7434372bc7d9241fd3b137b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d090f0639b8949af97a275f162737e91","IPY_MODEL_73da9d77b7b34c839ef7a7cbd3e96303"]}},"3a58d58dc7434372bc7d9241fd3b137b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d090f0639b8949af97a275f162737e91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0318336fe3f34cf0a9ccb482ff959283","_dom_classes":[],"description":" 43%","_model_name":"FloatProgressModel","bar_style":"danger","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4256,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a6e724044034c97a13f231c619937ca"}},"73da9d77b7b34c839ef7a7cbd3e96303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bf118b6ad810483987ec44edc70cf923","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4256/10000 [00:00&lt;00:00, 42557.50 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18d4013e177c4be1989f49d3e430866f"}},"0318336fe3f34cf0a9ccb482ff959283":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5a6e724044034c97a13f231c619937ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf118b6ad810483987ec44edc70cf923":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18d4013e177c4be1989f49d3e430866f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVpBmTS2QxV6","executionInfo":{"status":"ok","timestamp":1623578382502,"user_tz":-540,"elapsed":94646,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"a6304dd9-d24f-412a-fd37-af9b60029928"},"source":["!pip install \"git+https://github.com/google/uncertainty-baselines.git#egg=uncertainty_baselines\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting uncertainty_baselines\n","  Cloning https://github.com/google/uncertainty-baselines.git to /tmp/pip-install-xew90se9/uncertainty-baselines\n","  Running command git clone -q https://github.com/google/uncertainty-baselines.git /tmp/pip-install-xew90se9/uncertainty-baselines\n","Requirement already satisfied: absl-py>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (0.12.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.6.3)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.0.4)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.12)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (2.10)\n","Collecting ml_collections\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/d4/9ab1a8c2aebf78c348404c464733974dc4e7088174d6272ed09c2fa5a8fa/ml_collections-0.1.0-py3-none-any.whl (88kB)\n","\u001b[K     |████████████████████████████████| 92kB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.19.5)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.3.0)\n","Collecting tb-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/6b/ad3f7013014eb2e93577e1b9ce572bf41c9a72f8abe1b3d7ee965e8e601a/tb_nightly-2.6.0a20210612-py3-none-any.whl (5.5MB)\n","\u001b[K     |████████████████████████████████| 5.5MB 14.5MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (4.0.1)\n","Collecting tf-nightly\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/20/67/14fb4996bf25da88e536dc54b0361262578af2c1ea58626f50fca5796b97/tf_nightly-2.6.0.dev20210613-cp37-cp37m-manylinux2010_x86_64.whl\u001b[0m\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/67/14fb4996bf25da88e536dc54b0361262578af2c1ea58626f50fca5796b97/tf_nightly-2.6.0.dev20210613-cp37-cp37m-manylinux2010_x86_64.whl (454.6MB)\n","\u001b[K     |████████████████████████████████| 454.7MB 37kB/s \n","\u001b[?25hCollecting tfa-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/cf/07d41d2803d41000d15859115f63b9df32b23629681a9fccb117357235e9/tfa_nightly-0.14.0.dev20210605230927-cp37-cp37m-manylinux2010_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 32.3MB/s \n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.24.3)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.8.1->uncertainty_baselines) (1.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->uncertainty_baselines) (0.36.2)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml_collections->uncertainty_baselines) (0.5.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml_collections->uncertainty_baselines) (3.13)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.30.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (3.12.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (3.3.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.34.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (0.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (57.0.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (0.6.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.1.6)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (21.2.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (4.41.1)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (1.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.3.3)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (5.1.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.16.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (2.3)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (3.7.4.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (1.1.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (0.2.0)\n","Collecting tf-estimator-nightly~=2.5.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/6c/9bf4a6004d18c8e543845d3416e50f36dd09d272161e2fb0db5678132dfd/tf_estimator_nightly-2.5.0.dev2021032601-py2.py3-none-any.whl (462kB)\n","\u001b[K     |████████████████████████████████| 471kB 36.0MB/s \n","\u001b[?25hCollecting keras-nightly~=2.6.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/55/58a1159b23af2e9e92749995ed88159958c155d2efc5163fe0934b26e068/keras_nightly-2.6.0.dev2021061300-py2.py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 30.3MB/s \n","\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (0.4.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (3.1.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (1.12.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tfa-nightly->uncertainty_baselines) (2.7.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (4.2.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->uncertainty_baselines) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->uncertainty_baselines) (1.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly->uncertainty_baselines) (2020.12.5)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->uncertainty_baselines) (1.53.0)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly->uncertainty_baselines) (1.5.2)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->uncertainty_baselines) (3.1.0)\n","Building wheels for collected packages: uncertainty-baselines\n","  Building wheel for uncertainty-baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for uncertainty-baselines: filename=uncertainty_baselines-0.0.7-cp37-none-any.whl size=276282 sha256=87d99e4476ed6522d9c26ae84d0b36e98020deba605b33b67279ccb6eb49c40e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rykcsq11/wheels/5b/77/8b/8954f4644619426772bf3f47cb2246de4be5409186b43b0264\n","Successfully built uncertainty-baselines\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement keras-nightly~=2.5.0.dev, but you'll have keras-nightly 2.6.0.dev2021061300 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tf-nightly 2.6.0.dev20210613 has requirement grpcio<2.0,>=1.37.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n","Installing collected packages: ml-collections, tb-nightly, tf-estimator-nightly, keras-nightly, tf-nightly, tfa-nightly, uncertainty-baselines\n","  Found existing installation: keras-nightly 2.5.0.dev2021032900\n","    Uninstalling keras-nightly-2.5.0.dev2021032900:\n","      Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n","Successfully installed keras-nightly-2.6.0.dev2021061300 ml-collections-0.1.0 tb-nightly-2.6.0a20210612 tf-estimator-nightly-2.5.0.dev2021032601 tf-nightly-2.6.0.dev20210613 tfa-nightly-0.14.0.dev20210605230927 uncertainty-baselines-0.0.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrsFLW26bXJY","executionInfo":{"status":"ok","timestamp":1623578394773,"user_tz":-540,"elapsed":12284,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"02fef4a5-7071-45ac-b20b-167e24314706"},"source":["pip install \"git+https://github.com/google-research/robustness_metrics.git#egg=robustness_metrics\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting robustness_metrics\n","  Cloning https://github.com/google-research/robustness_metrics.git to /tmp/pip-install-8pzpztdc/robustness-metrics\n","  Running command git clone -q https://github.com/google-research/robustness_metrics.git /tmp/pip-install-8pzpztdc/robustness-metrics\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.12.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (1.1.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.22.2.post1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.8.9)\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (4.0.1)\n","Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.12.0)\n","Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (2.6.0.dev20210613)\n","Collecting tfp-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/99/e1b288cab9b6e689da7ff935327b4c439353ec1ebc1f73f34ddfa18584fa/tfp_nightly-0.14.0.dev20210613-py2.py3-none-any.whl (5.5MB)\n","\u001b[K     |████████████████████████████████| 5.5MB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->robustness_metrics) (1.15.0)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (2018.9)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->robustness_metrics) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->robustness_metrics) (1.4.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (1.1.0)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (5.1.3)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (21.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.16.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (1.0.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.1.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (4.41.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (2.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.3.3)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (3.12.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (2.23.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.36.2)\n","Requirement already satisfied: tb-nightly~=2.6.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.6.0a20210612)\n","Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.5.0.dev2021032601)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.1.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.2.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.6.3)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.7.4.3)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.3.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.12)\n","Requirement already satisfied: keras-nightly~=2.6.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.6.0.dev2021061300)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.1.2)\n","Collecting grpcio<2.0,>=1.37.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/af/ecae3a21bed5a92c9d866480553efea18a39f103546108cd60f5ea6a2494/grpcio-1.38.0-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n","\u001b[K     |████████████████████████████████| 4.2MB 35.4MB/s \n","\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.12.1)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tfp-nightly->robustness_metrics) (1.3.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tfp-nightly->robustness_metrics) (4.4.2)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->robustness_metrics) (3.4.1)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->robustness_metrics) (1.53.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow_datasets->robustness_metrics) (57.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (1.24.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.30.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.8.0)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly->robustness_metrics) (1.5.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (3.1.0)\n","Building wheels for collected packages: robustness-metrics\n","  Building wheel for robustness-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for robustness-metrics: filename=robustness_metrics-0.0.1-cp37-none-any.whl size=99579 sha256=87c61572c432b9d683557f588ff5149b77788ece857e6e98bee9bec8c8319844\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uf_rfer9/wheels/f1/09/d6/441f73c886118e9f7afb6c7c15f0273279567036d7390e345d\n","Successfully built robustness-metrics\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement grpcio~=1.34.0, but you'll have grpcio 1.38.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement keras-nightly~=2.5.0.dev, but you'll have keras-nightly 2.6.0.dev2021061300 which is incompatible.\u001b[0m\n","Installing collected packages: tfp-nightly, robustness-metrics, grpcio\n","  Found existing installation: grpcio 1.34.1\n","    Uninstalling grpcio-1.34.1:\n","      Successfully uninstalled grpcio-1.34.1\n","Successfully installed grpcio-1.38.0 robustness-metrics-0.0.1 tfp-nightly-0.14.0.dev20210613\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tUhcLEebZny","executionInfo":{"status":"ok","timestamp":1623578401876,"user_tz":-540,"elapsed":7108,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"a9c385f5-6909-4f87-c4e7-c412bbba95de"},"source":["pip install \"git+https://github.com/google/edward2\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/google/edward2\n","  Cloning https://github.com/google/edward2 to /tmp/pip-req-build-xnwbbivr\n","  Running command git clone -q https://github.com/google/edward2 /tmp/pip-req-build-xnwbbivr\n","Building wheels for collected packages: edward2\n","  Building wheel for edward2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for edward2: filename=edward2-0.0.3-cp37-none-any.whl size=168433 sha256=da02f58bfbb7bd003e255848466f69e5e90f4091ccc30ea07039696e67600934\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-g5v4kq21/wheels/f9/fb/ed/96a1b2e305c0df592e14b8a7bf576b09f1540542771856eb15\n","Successfully built edward2\n","Installing collected packages: edward2\n","Successfully installed edward2-0.0.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nY10M9WKNwBB"},"source":["#시작"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CoeNznmInaA","executionInfo":{"status":"ok","timestamp":1623578482274,"user_tz":-540,"elapsed":80401,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"5ec79ad3-3584-4fa4-c336-2fc911818439"},"source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dhzxWl_2Ip1D","executionInfo":{"status":"ok","timestamp":1623578482275,"user_tz":-540,"elapsed":11,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["# import pandas as pd\n","# df = pd.read_csv('/content/gdrive/My Drive/tmp/cifar')\n","# print(len(df))\n","# df.head()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":76},"id":"P-W9SXgTOA1u","executionInfo":{"status":"ok","timestamp":1623578492859,"user_tz":-540,"elapsed":10593,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"1fb882c0-5fa0-4edf-c5f2-86373bcb7da3"},"source":["from google.colab import files\n","src = list(files.upload().values())[0]\n","open('utils.py','wb').write(src)\n","import utils"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-7d420789-e45d-4ed4-9b85-77bd677a7ecd\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7d420789-e45d-4ed4-9b85-77bd677a7ecd\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving utils.py to utils.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5oRio5jMcvJW"},"source":["# cifar 데이터셋"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6lISZ6rZ43e","executionInfo":{"status":"ok","timestamp":1623578502825,"user_tz":-540,"elapsed":9968,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"aaa795d4-73ca-4d5b-ae13-098ee0f1ebab"},"source":["#cifar데이터셋\n","\n","# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\"\"\"CIFAR{10,100} dataset builders.\"\"\"\n","\n","from typing import Any, Dict, Optional, Union\n","\n","from robustness_metrics.common import types\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets as tfds\n","from uncertainty_baselines.datasets import augment_utils\n","from uncertainty_baselines.datasets import augmix\n","from uncertainty_baselines.datasets import base\n","\n","# We use the convention of using mean = np.mean(train_images, axis=(0,1,2))\n","# and std = np.std(train_images, axis=(0,1,2)).\n","CIFAR10_MEAN = tf.constant([0.4914, 0.4822, 0.4465])\n","CIFAR10_STD = tf.constant([0.2470, 0.2435, 0.2616])\n","# Previously we used std = np.mean(np.std(train_images, axis=(1, 2)), axis=0)\n","# which gave std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype), however\n","# we change convention to use the std over the entire training set instead.\n","\n","\n","def _tuple_dict_fn_converter(fn, *args):\n","\n","  def dict_fn(batch_dict):\n","    images, labels = fn(*args, batch_dict['features'], batch_dict['labels'])\n","    return {'features': images, 'labels': labels}\n","\n","  return dict_fn\n","\n","\n","class _CifarDataset(base.BaseDataset):\n","  \"\"\"CIFAR dataset builder abstract class.\"\"\"\n","\n","  def __init__(\n","      self,\n","      name: str,\n","      fingerprint_key: str,\n","      split: str,\n","      seed: Optional[Union[int, tf.Tensor]] = None,\n","      validation_percent: float = 0.0,\n","      shuffle_buffer_size: Optional[int] = None,\n","      num_parallel_parser_calls: int = 64,\n","      drop_remainder: bool = True,\n","      normalize: bool = True,\n","      try_gcs: bool = False,\n","      download_data: bool = False,\n","      use_bfloat16: bool = False,\n","      aug_params: Optional[Dict[str, Any]] = None,\n","      data_dir: Optional[str] = None,\n","      is_training: Optional[bool] = None,\n","      **unused_kwargs: Dict[str, Any]):\n","    \"\"\"Create a CIFAR10 or CIFAR100 tf.data.Dataset builder.\n","    Args:\n","      name: the name of this dataset, either 'cifar10' or 'cifar100'.\n","      fingerprint_key: The name of the feature holding a string that will be\n","        used to create an element id using a fingerprinting function. If None,\n","        then `ds.enumerate()` is added before the `ds.map(preprocessing_fn)` is\n","        called and an `id` field is added to the example Dict.\n","      split: a dataset split, either a custom tfds.Split or one of the\n","        tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n","        names.\n","      seed: the seed used as a source of randomness.\n","      validation_percent: the percent of the training set to use as a validation\n","        set.\n","      shuffle_buffer_size: the number of example to use in the shuffle buffer\n","        for tf.data.Dataset.shuffle().\n","      num_parallel_parser_calls: the number of parallel threads to use while\n","        preprocessing in tf.data.Dataset.map().\n","      drop_remainder: whether or not to drop the last batch of data if the\n","        number of points is not exactly equal to the batch size. This option\n","        needs to be True for running on TPUs.\n","      normalize: whether or not to normalize each image by the CIFAR dataset\n","        mean and stddev.\n","      try_gcs: Whether or not to try to use the GCS stored versions of dataset\n","        files.\n","      download_data: Whether or not to download data before loading.\n","      use_bfloat16: Whether or not to load the data in bfloat16 or float32.\n","      aug_params: hyperparameters for the data augmentation pre-processing.\n","      data_dir: Directory to read/write data, that is passed to the\n","        tfds dataset_builder as a data_dir parameter.\n","      is_training: Whether or not the given `split` is the training split. Only\n","        required when the passed split is not one of ['train', 'validation',\n","        'test', tfds.Split.TRAIN, tfds.Split.VALIDATION, tfds.Split.TEST].\n","    \"\"\"\n","    self._normalize = normalize\n","    dataset_builder = tfds.builder(\n","        name, \n","        #try_gcs=try_gcs,\n","        data_dir=data_dir)\n","    if is_training is None:\n","      is_training = split in ['train', tfds.Split.TRAIN]\n","    new_split = base.get_validation_percent_split(\n","        dataset_builder, validation_percent, split)\n","    super(_CifarDataset, self).__init__(\n","        name=name,\n","        dataset_builder=dataset_builder,\n","        split=new_split,\n","        seed=seed,\n","        is_training=is_training,\n","        shuffle_buffer_size=shuffle_buffer_size,\n","        num_parallel_parser_calls=num_parallel_parser_calls,\n","        drop_remainder=drop_remainder,\n","        fingerprint_key=fingerprint_key,\n","        download_data=download_data,\n","        cache=True)\n","\n","    self._use_bfloat16 = use_bfloat16\n","    if aug_params is None:\n","      aug_params = {}\n","    self._adaptive_mixup = aug_params.get('adaptive_mixup', False)\n","    ensemble_size = aug_params.get('ensemble_size', 1)\n","    if self._adaptive_mixup and 'mixup_coeff' not in aug_params:\n","      # Hard target in the first epoch!\n","      aug_params['mixup_coeff'] = tf.ones([ensemble_size, 10])\n","    self._aug_params = aug_params\n","\n","  def _create_process_example_fn(self) -> base.PreProcessFn:\n","\n","    def _example_parser(example: types.Features) -> types.Features:\n","      \"\"\"A pre-process function to return images in [0, 1].\"\"\"\n","      image = example['image']\n","      image_dtype = tf.bfloat16 if self._use_bfloat16 else tf.float32\n","      use_augmix = self._aug_params.get('augmix', False)\n","      if self._is_training:\n","        image_shape = tf.shape(image)\n","        # Expand the image by 2 pixels, then crop back down to 32x32.\n","        image = tf.image.resize_with_crop_or_pad(\n","            image, image_shape[0] + 4, image_shape[1] + 4)\n","        # Note that self._seed will already be shape (2,), as is required for\n","        # stateless random ops, and so will per_example_step_seed.\n","        per_example_step_seed = tf.random.experimental.stateless_fold_in(\n","            self._seed, example[self._enumerate_id_key])\n","        # per_example_step_seeds will be of size (num, 3).\n","        # First for random_crop, second for flip, third optionally for\n","        # RandAugment, and foruth optionally for Augmix.\n","        per_example_step_seeds = tf.random.experimental.stateless_split(\n","            per_example_step_seed, num=4)\n","        image = tf.image.stateless_random_crop(\n","            image,\n","            (image_shape[0], image_shape[0], 3),\n","            seed=per_example_step_seeds[0])\n","        image = tf.image.stateless_random_flip_left_right(\n","            image,\n","            seed=per_example_step_seeds[1])\n","\n","        # Only random augment for now.\n","        if self._aug_params.get('random_augment', False):\n","          count = self._aug_params['aug_count']\n","          augment_seeds = tf.random.experimental.stateless_split(\n","              per_example_step_seeds[2], num=count)\n","          augmenter = augment_utils.RandAugment()\n","          augmented = [\n","              augmenter.distort(image, seed=augment_seeds[c])\n","              for c in range(count)\n","          ]\n","          image = tf.stack(augmented)\n","\n","        if use_augmix:\n","          augmenter = augment_utils.RandAugment()\n","          image = augmix.do_augmix(\n","              image, self._aug_params, augmenter, image_dtype,\n","              mean=CIFAR10_MEAN, std=CIFAR10_STD,\n","              seed=per_example_step_seeds[3])\n","\n","      # The image has values in the range [0, 1].\n","      # Optionally normalize by the dataset statistics.\n","      if not use_augmix:\n","        if self._normalize:\n","          image = augmix.normalize_convert_image(\n","              image, image_dtype, mean=CIFAR10_MEAN, std=CIFAR10_STD)\n","        else:\n","          image = tf.image.convert_image_dtype(image, image_dtype)\n","      parsed_example = example.copy()\n","      parsed_example['features'] = image\n","\n","      # Note that labels are always float32, even when images are bfloat16.\n","      mixup_alpha = self._aug_params.get('mixup_alpha', 0)\n","      label_smoothing = self._aug_params.get('label_smoothing', 0.)\n","      should_onehot = mixup_alpha > 0 or label_smoothing > 0\n","      if should_onehot:\n","        parsed_example['labels'] = tf.one_hot(\n","            example['label'], 10, dtype=tf.float32)\n","      else:\n","        parsed_example['labels'] = tf.cast(example['label'], tf.float32)\n","\n","      del parsed_example['image']\n","      del parsed_example['label']\n","      return parsed_example\n","\n","    return _example_parser\n","\n","  def _create_process_batch_fn(\n","      self,\n","      batch_size: int) -> Optional[base.PreProcessFn]:\n","    if self._is_training and self._aug_params.get('mixup_alpha', 0) > 0:\n","      if self._adaptive_mixup:\n","        return _tuple_dict_fn_converter(\n","            augmix.adaptive_mixup, batch_size, self._aug_params)\n","      else:\n","        return _tuple_dict_fn_converter(\n","            augmix.mixup, batch_size, self._aug_params)\n","    return None\n","\n","\n","class Cifar10Dataset(_CifarDataset):\n","  \"\"\"CIFAR10 dataset builder class.\"\"\"\n","\n","  def __init__(self, **kwargs):\n","    super(Cifar10Dataset, self).__init__(\n","        name='cifar10',\n","        fingerprint_key='id',\n","        **kwargs)\n","\n","\n","class Cifar100Dataset(_CifarDataset):\n","  \"\"\"CIFAR100 dataset builder class.\"\"\"\n","\n","  def __init__(self, **kwargs):\n","    super(Cifar100Dataset, self).__init__(\n","        name='cifar100',\n","        fingerprint_key='id',\n","        **kwargs)\n","\n","\n","class Cifar10CorruptedDataset(_CifarDataset):\n","  \"\"\"CIFAR10-C dataset builder class.\"\"\"\n","\n","  def __init__(\n","      self,\n","      corruption_type: str,\n","      severity: int,\n","      **kwargs):\n","    \"\"\"Create a CIFAR10-C tf.data.Dataset builder.\n","    Args:\n","      corruption_type: Corruption name.\n","      severity: Corruption severity, an integer between 1 and 5.\n","      **kwargs: Additional keyword arguments.\n","    \"\"\"\n","    super(Cifar10CorruptedDataset, self).__init__(\n","        name=f'cifar10_corrupted/{corruption_type}_{severity}',\n","        fingerprint_key=None,\n","        **kwargs)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/edward2/__init__.py:34: UserWarning: JAX backend for Edward2 is not available.\n","  warnings.warn(\"JAX backend for Edward2 is not available.\")\n","/usr/local/lib/python3.7/dist-packages/uncertainty_baselines/models/models.py:39: UserWarning: Skipped due to ImportError: No module named 'official'\n","  warnings.warn(f'Skipped due to ImportError: {e}')\n","/usr/local/lib/python3.7/dist-packages/uncertainty_baselines/models/__init__.py:67: UserWarning: Skipped ViT models due to ImportError: No module named 'flax'\n","  warnings.warn(f'Skipped ViT models due to ImportError: {e}')\n","/usr/local/lib/python3.7/dist-packages/uncertainty_baselines/models/__init__.py:76: UserWarning: Skipped BERT models due to ImportError: No module named 'official'\n","  warnings.warn(f'Skipped BERT models due to ImportError: {e}')\n","/usr/local/lib/python3.7/dist-packages/uncertainty_baselines/models/__init__.py:84: UserWarning: Skipped MIMO models due to ImportError: No module named 'edward2.experimental'\n","  warnings.warn(f'Skipped MIMO models due to ImportError: {e}')\n","/usr/local/lib/python3.7/dist-packages/uncertainty_baselines/models/__init__.py:93: UserWarning: Skipped importing Flax ViT models due to ImportError: No module named 'flax'\n","  warnings.warn(f'Skipped importing Flax ViT models due to ImportError: {e}')\n","/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.6.0-dev20210613). \n","TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n","If you encounter a bug, do not file an issue on GitHub.\n","  UserWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bKR3NGUEcyQm"},"source":["# datasets"]},{"cell_type":"code","metadata":{"id":"iV-aaJyuacx1","executionInfo":{"status":"ok","timestamp":1623578502826,"user_tz":-540,"elapsed":6,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["#datasets\n","\n","# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","# Lint as: python3\n","\"\"\"Dataset getter utility.\"\"\"\n","\n","import json\n","import logging\n","from typing import Any, List, Tuple, Union\n","import warnings\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from uncertainty_baselines.datasets.base import BaseDataset\n","from uncertainty_baselines.datasets.cifar import Cifar100Dataset\n","from uncertainty_baselines.datasets.cifar import Cifar10CorruptedDataset\n","#from uncertainty_baselines.datasets.cifar import Cifar10Dataset\n","from uncertainty_baselines.datasets.cifar100_corrupted import Cifar100CorruptedDataset\n","from uncertainty_baselines.datasets.clinc_intent import ClincIntentDetectionDataset\n","from uncertainty_baselines.datasets.criteo import CriteoDataset\n","from uncertainty_baselines.datasets.diabetic_retinopathy_detection import DiabeticRetinopathyDetectionDataset\n","from uncertainty_baselines.datasets.genomics_ood import GenomicsOodDataset\n","from uncertainty_baselines.datasets.glue import GlueDatasets\n","from uncertainty_baselines.datasets.imagenet import ImageNetDataset\n","from uncertainty_baselines.datasets.mnist import MnistDataset\n","from uncertainty_baselines.datasets.mnli import MnliDataset\n","from uncertainty_baselines.datasets.movielens import MovieLensDataset\n","from uncertainty_baselines.datasets.places import Places365Dataset\n","from uncertainty_baselines.datasets.random import RandomGaussianImageDataset\n","from uncertainty_baselines.datasets.random import RandomRademacherImageDataset\n","from uncertainty_baselines.datasets.svhn import SvhnDataset\n","from uncertainty_baselines.datasets.toxic_comments import CivilCommentsDataset\n","from uncertainty_baselines.datasets.toxic_comments import CivilCommentsIdentitiesDataset\n","from uncertainty_baselines.datasets.toxic_comments import WikipediaToxicityDataset\n","\n","try:\n","  from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n","except ImportError as e:\n","  warnings.warn(f'Skipped due to ImportError: {e}')\n","  SpeechCommandsDataset = None\n","\n","DATASETS = {\n","    'cifar100': Cifar100Dataset,\n","    'cifar10': Cifar10Dataset,\n","    'cifar10_corrupted': Cifar10CorruptedDataset,\n","    'cifar100_corrupted': Cifar100CorruptedDataset,\n","    'civil_comments': CivilCommentsDataset,\n","    'civil_comments_identities': CivilCommentsIdentitiesDataset,\n","    'clinic_intent': ClincIntentDetectionDataset,\n","    'criteo': CriteoDataset,\n","    'diabetic_retinopathy_detection': DiabeticRetinopathyDetectionDataset,\n","    'imagenet': ImageNetDataset,\n","    'mnist': MnistDataset,\n","    'mnli': MnliDataset,\n","    'movielens': MovieLensDataset,\n","    'places365': Places365Dataset,\n","    'random_gaussian': RandomGaussianImageDataset,\n","    'random_rademacher': RandomRademacherImageDataset,\n","    'speech_commands': SpeechCommandsDataset,\n","    'svhn_cropped': SvhnDataset,\n","    'glue/cola': GlueDatasets['glue/cola'],\n","    'glue/sst2': GlueDatasets['glue/sst2'],\n","    'glue/mrpc': GlueDatasets['glue/mrpc'],\n","    'glue/qqp': GlueDatasets['glue/qqp'],\n","    'glue/qnli': GlueDatasets['glue/qnli'],\n","    'glue/rte': GlueDatasets['glue/rte'],\n","    'glue/wnli': GlueDatasets['glue/wnli'],\n","    'glue/stsb': GlueDatasets['glue/stsb'],\n","    'wikipedia_toxicity': WikipediaToxicityDataset,\n","    'genomics_ood': GenomicsOodDataset,\n","}\n","\n","\n","def get_dataset_names() -> List[str]:\n","  return list(DATASETS.keys())\n","\n","\n","def get(\n","    dataset_name: str,\n","    split: Union[Tuple[str, float], str, tfds.Split],\n","    **hyperparameters: Any) -> BaseDataset:\n","  \"\"\"Gets a dataset builder by name.\n","  Note that the user still needs to call\n","  `distribution_strategy.experimental_distribute_dataset(dataset)` on the loaded\n","  dataset if they are running in a distributed environment.\n","  Args:\n","    dataset_name: Name of the dataset builder class.\n","    split: a dataset split, either a custom tfds.Split or one of the\n","      tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n","      names.\n","    **hyperparameters: dict of possible kwargs to be passed to the dataset\n","      constructor.\n","  Returns:\n","    A dataset builder class with a method .build(split) which can be called to\n","    get the tf.data.Dataset, which has elements that are a dict described by\n","    dataset_builder.info.\n","  Raises:\n","    ValueError: If dataset_name is unrecognized.\n","  \"\"\"\n","  hyperparameters_py = {\n","      k: (v.numpy().tolist() if isinstance(v, tf.Tensor) else v)\n","      for k, v in hyperparameters.items()\n","  }\n","  logging.info(\n","      'Building dataset %s with additional kwargs:\\n%s',\n","      dataset_name,\n","      json.dumps(hyperparameters_py, indent=2, sort_keys=True))\n","  if dataset_name not in DATASETS:\n","    raise ValueError('Unrecognized dataset name: {!r}'.format(dataset_name))\n","\n","  dataset_class = DATASETS[dataset_name]\n","  return dataset_class(\n","      split=split,\n","      **hyperparameters)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rWVYr4WgNAU","colab":{"base_uri":"https://localhost:8080/","height":358,"referenced_widgets":["2cb6b90b477b438ba2af846e0cabab0c","76792b37395c4136ac3b7ce5a55298d8","b5485aec2d834159822729e410c13bc9","21631d2547c14da9a81ebdd668d3530f","17703acd868f402f9c42b7183eabb27c","eb1174692fff400ca259932e8de6ab80","a4e51d076c38451e9013bab165b82492","3df760f7e978479f85b8373e491d461b","0b6c892e174142698f9bf6a0a120f3b7","4c9c2fbd77c3432282a8b2299662fcff","a1dab66868254c42acc64128809f2bac","d72c7e2fbf9644eda4fb558486d245ee","ba303995f9d841e9ada3d1c409bb3c58","8dba1c4bfca34a95b5cc43bec59f6fc7","85569d53987547b8a379035877742146","5574b1dddd22410eae0534436f0e49c7","62e5245d8d9c455bb48b006d0433a377","1b9b940ac62441edb9200dca3e1b2def","be6c6d2d8a144c768670a998e3e44f49","47be908210af4b04ab73ec7a66118a89","b98391be273e4680a729a52f7a9a052d","12bd1070db1e429cac60031aba072d40","061337a0e2a34af4a7bc2aea00e7d52d","bd7bb6165bfa4122ba0758befcbd1cff","0934e9a8c1444c6baeb3e1748c968b8b","bd5527beca1a4e58bff2a18202171f16","62853fb5c3e142f8832ee280b58f474e","023c8d471485427ca561c9f3250575cd","5e0a1070e90e4d1a93d5dd22702d21ad","2ca8eccaacac458ba5a617c6606d988e","5a727af553c94444aa786a198f738947","f52a1311f11f4512885776216f30f4dc","9cf79ac5d390414a84d1160a09f67d86","ec656655591544d5998327bf54096e7d","ea393c44c4eb44fe806aa96393a4dac5","b5f99d7cb7014d31902597db4cf3a305","ef3f8567cf8c4dbe8913b7be22b313c3","5dfad64b7e214402ab296cb552a6b12d","b00b17562f1e4387ae0f3e9adad170e4","240370c6ee464cfdb0cc87322a96e012","1a4910d538f247fc8830804295c17e30","b406825416034f37aa4f235092bfb629","40f08bf714e647569de204c1c5d5579d","35143605a3f24bdba281b24ee2fbcc25","4e3b92c013fc488eafb639e72e99b3ef","099ed082d9d8402b95e52c723370bf2a","7328765b790940b5bab55bdf5704a9b6","67a37925c5694f208e82f56cb8818a36","55c24605a1194bb2b35e9866c8d27b85","3a58d58dc7434372bc7d9241fd3b137b","d090f0639b8949af97a275f162737e91","73da9d77b7b34c839ef7a7cbd3e96303","0318336fe3f34cf0a9ccb482ff959283","5a6e724044034c97a13f231c619937ca","bf118b6ad810483987ec44edc70cf923","18d4013e177c4be1989f49d3e430866f"]},"executionInfo":{"status":"ok","timestamp":1623578559618,"user_tz":-540,"elapsed":56797,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"b2947710-8eed-43aa-956f-8daee72b5237"},"source":["dataset_builder = tfds.builder('cifar10')\n","dataset_builder.download_and_prepare()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset cifar10/3.0.2 (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /root/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cb6b90b477b438ba2af846e0cabab0c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b6c892e174142698f9bf6a0a120f3b7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62e5245d8d9c455bb48b006d0433a377","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0934e9a8c1444c6baeb3e1748c968b8b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/cifar10/3.0.2.incompleteIKPCNE/cifar10-train.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cf79ac5d390414a84d1160a09f67d86","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a4910d538f247fc8830804295c17e30","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/cifar10/3.0.2.incompleteIKPCNE/cifar10-test.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55c24605a1194bb2b35e9866c8d27b85","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1mDataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l20R8Qegh9a-"},"source":["# 모델"]},{"cell_type":"code","metadata":{"id":"uLoqWHV1h6BZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623578560174,"user_tz":-540,"elapsed":568,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"9697aaaa-f12d-42e2-9709-2424107f022d"},"source":["# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","# Lint as: python3\n","\"\"\"Wide Residual Network.\"\"\"\n","\n","import functools\n","from typing import Any, Dict, Iterable, Optional\n","\n","import tensorflow as tf\n","\n","HP_KEYS = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","\n","BatchNormalization = functools.partial(  # pylint: disable=invalid-name\n","    tf.keras.layers.BatchNormalization,\n","    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n","    momentum=0.9)\n","\n","\n","def Conv2D(filters, seed=None, **kwargs):  # pylint: disable=invalid-name\n","  \"\"\"Conv2D layer that is deterministically initialized.\"\"\"\n","  default_kwargs = {\n","      'kernel_size': 3,\n","      'padding': 'same',\n","      'use_bias': False,\n","      # Note that we need to use the class constructor for the initializer to\n","      # get deterministic initialization.\n","      'kernel_initializer': tf.keras.initializers.HeNormal(seed=seed),\n","  }\n","  # Override defaults with the passed kwargs.\n","  default_kwargs.update(kwargs)\n","  return tf.keras.layers.Conv2D(filters, **default_kwargs)\n","\n","\n","def basic_block(\n","    inputs: tf.Tensor,\n","    filters: int,\n","    strides: int,\n","    conv_l2: float,\n","    bn_l2: float,\n","    seed: int,\n","    version: int) -> tf.Tensor:\n","  \"\"\"Basic residual block of two 3x3 convs.\n","  Args:\n","    inputs: tf.Tensor.\n","    filters: Number of filters for Conv2D.\n","    strides: Stride dimensions for Conv2D.\n","    conv_l2: L2 regularization coefficient for the conv kernels.\n","    bn_l2: L2 regularization coefficient for the batch norm layers.\n","    seed: random seed used for initialization.\n","    version: 1, indicating the original ordering from He et al. (2015); or 2,\n","      indicating the preactivation ordering from He et al. (2016).\n","  Returns:\n","    tf.Tensor.\n","  \"\"\"\n","  x = inputs\n","  y = inputs\n","  if version == 2:\n","    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                           gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","    y = tf.keras.layers.Activation('relu')(y)\n","  seeds = tf.random.experimental.stateless_split([seed, seed + 1], 3)[:, 0]\n","  y = Conv2D(filters,\n","             strides=strides,\n","             seed=seeds[0],\n","             kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(y)\n","  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                         gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","  y = tf.keras.layers.Activation('relu')(y)\n","  y = Conv2D(filters,\n","             strides=1,\n","             seed=seeds[1],\n","             kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(y)\n","  if version == 1:\n","    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                           gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","  if not x.shape.is_compatible_with(y.shape):\n","    x = Conv2D(filters,\n","               kernel_size=1,\n","               strides=strides,\n","               seed=seeds[2],\n","               kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(x)\n","  x = tf.keras.layers.add([x, y])\n","  if version == 1:\n","    x = tf.keras.layers.Activation('relu')(x)\n","  return x\n","\n","\n","def group(inputs, filters, strides, num_blocks, conv_l2, bn_l2, version, seed):\n","  \"\"\"Group of residual blocks.\"\"\"\n","  seeds = tf.random.experimental.stateless_split(\n","      [seed, seed + 1], num_blocks)[:, 0]\n","  x = basic_block(\n","      inputs,\n","      filters=filters,\n","      strides=strides,\n","      conv_l2=conv_l2,\n","      bn_l2=bn_l2,\n","      version=version,\n","      seed=seeds[0])\n","  for i in range(num_blocks - 1):\n","    x = basic_block(\n","        x,\n","        filters=filters,\n","        strides=1,\n","        conv_l2=conv_l2,\n","        bn_l2=bn_l2,\n","        version=version,\n","        seed=seeds[i + 1])\n","  return x\n","\n","\n","def _parse_hyperparameters(l2: float, hps: Dict[str, float]):\n","  \"\"\"Extract the L2 parameters for the dense, conv and batch-norm layers.\"\"\"\n","\n","  assert_msg = ('Ambiguous hyperparameter specifications: either l2 or hps '\n","                'must be provided (received {} and {}).'.format(l2, hps))\n","  is_specified = lambda h: bool(h) and all(v is not None for v in h.values())\n","  only_l2_is_specified = l2 is not None and not is_specified(hps)\n","  only_hps_is_specified = l2 is None and is_specified(hps)\n","  assert only_l2_is_specified or only_hps_is_specified, assert_msg\n","  if only_hps_is_specified:\n","    assert_msg = 'hps must contain the keys {}!={}.'.format(HP_KEYS, hps.keys())\n","    assert set(hps.keys()).issuperset(HP_KEYS), assert_msg\n","    return hps\n","  else:\n","    return {k: l2 for k in HP_KEYS}\n","\n","\n","def wide_resnet(\n","    input_shape: Iterable[int],\n","    depth: int,\n","    width_multiplier: int,\n","    num_classes: int,\n","    l2: float,\n","    version: int = 2,\n","    seed: int = 42,\n","    hps: Optional[Dict[str, float]] = None) -> tf.keras.models.Model:\n","  \"\"\"Builds Wide ResNet.\n","  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n","  number of filters. Using three groups of residual blocks, the network maps\n","  spatial features of size 32x32 -> 16x16 -> 8x8.\n","  Args:\n","    input_shape: tf.Tensor.\n","    depth: Total number of convolutional layers. \"n\" in WRN-n-k. It differs from\n","      He et al. (2015)'s notation which uses the maximum depth of the network\n","      counting non-conv layers like dense.\n","    width_multiplier: Integer to multiply the number of typical filters by. \"k\"\n","      in WRN-n-k.\n","    num_classes: Number of output classes.\n","    l2: L2 regularization coefficient.\n","    version: 1, indicating the original ordering from He et al. (2015); or 2,\n","      indicating the preactivation ordering from He et al. (2016).\n","    seed: random seed used for initialization.\n","    hps: Fine-grained specs of the hyperparameters, as a Dict[str, float].\n","  Returns:\n","    tf.keras.Model.\n","  \"\"\"\n","  l2_reg = tf.keras.regularizers.l2\n","  hps = _parse_hyperparameters(l2, hps)\n","\n","  seeds = tf.random.experimental.stateless_split([seed, seed + 1], 5)[:, 0]\n","  if (depth - 4) % 6 != 0:\n","    raise ValueError('depth should be 6n+4 (e.g., 16, 22, 28, 40).')\n","  num_blocks = (depth - 4) // 6\n","  inputs = tf.keras.layers.Input(shape=input_shape)\n","  x = Conv2D(16,\n","             strides=1,\n","             seed=seeds[0],\n","             kernel_regularizer=l2_reg(hps['input_conv_l2']))(inputs)\n","  if version == 1:\n","    x = BatchNormalization(beta_regularizer=l2_reg(hps['bn_l2']),\n","                           gamma_regularizer=l2_reg(hps['bn_l2']))(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","  x = group(x,\n","            filters=16 * width_multiplier,\n","            strides=1,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_1_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[1])\n","  x = group(x,\n","            filters=32 * width_multiplier,\n","            strides=2,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_2_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[2])\n","  x = group(x,\n","            filters=64 * width_multiplier,\n","            strides=2,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_3_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[3])\n","  if version == 2:\n","    x = BatchNormalization(beta_regularizer=l2_reg(hps['bn_l2']),\n","                           gamma_regularizer=l2_reg(hps['bn_l2']))(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n","  x = tf.keras.layers.Flatten()(x)\n","  x = tf.keras.layers.Dense(\n","      num_classes,\n","      kernel_initializer=tf.keras.initializers.HeNormal(seed=seeds[4]),\n","      kernel_regularizer=l2_reg(hps['dense_kernel_l2']),\n","      bias_regularizer=l2_reg(hps['dense_bias_l2']))(x)\n","  return tf.keras.Model(\n","      inputs=inputs,\n","      outputs=x,\n","      name='wide_resnet-{}-{}'.format(depth, width_multiplier))\n","\n","\n","def create_model(\n","    batch_size: Optional[int],\n","    depth: int,\n","    width_multiplier: int,\n","    input_shape: Iterable[int] = (32, 32, 3),\n","    num_classes: int = 10,\n","    l2_weight: float = 0.0,\n","    version: int = 2,\n","    **unused_kwargs: Dict[str, Any]) -> tf.keras.models.Model:\n","  \"\"\"Creates model.\"\"\"\n","  del batch_size  # unused arg\n","  return wide_resnet(input_shape=input_shape,\n","                     depth=depth,\n","                     width_multiplier=width_multiplier,\n","                     num_classes=num_classes,\n","                     l2=l2_weight,\n","                     version=version)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\r"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9gVUYU9qlcOw","executionInfo":{"status":"ok","timestamp":1623578560174,"user_tz":-540,"elapsed":3,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["# try:\n","#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","#     print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","# except ValueError:\n","#     tpu = None\n","\n","# if tpu:\n","#     tf.config.experimental_connect_to_cluster(tpu)\n","#     tf.tpu.experimental.initialize_tpu_system(tpu)\n","#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","# else:\n","#     strategy = tf.distribute.get_strategy()\n","\n","# print(\"REPLICAS: \", strategy.num_replicas_in_sync)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZB84Bamhc2q_"},"source":["# 중요코드"]},{"cell_type":"code","metadata":{"id":"sSEUvyFtSJWy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea7c255f-78c0-4c45-f730-7805fb17be44"},"source":["# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\"\"\"Wide ResNet 28-10 on CIFAR-10/100 trained with maximum likelihood.\n","Hyperparameters differ slightly from the original paper's code\n","(https://github.com/szagoruyko/wide-residual-networks) as TensorFlow uses, for\n","example, l2 instead of weight decay, and a different parameterization for SGD's\n","momentum.\n","\"\"\"\n","\n","import os\n","import time\n","from absl import app\n","from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","flags.DEFINE_string(\"f\", \"\", \"kernel\")\n","flags.DEFINE_float('label_smoothing', 0., 'Label smoothing parameter in [0,1].')\n","flags.register_validator('label_smoothing',\n","                         lambda ls: ls >= 0.0 and ls <= 1.0,\n","                         message='--label_smoothing must be in [0, 1].')\n","\n","# Data Augmentation flags.\n","flags.DEFINE_bool('augmix', False,\n","                  'Whether to perform AugMix [4] on the input data.')\n","flags.DEFINE_integer('aug_count', 1,\n","                     'Number of augmentation operations in AugMix to perform '\n","                     'on the input image. In the simgle model context, it'\n","                     'should be 1. In the ensembles context, it should be'\n","                     'ensemble_size if we perform random_augment only; It'\n","                     'should be (ensemble_size - 1) if we perform augmix.')\n","flags.DEFINE_float('augmix_prob_coeff', 0.5, 'Augmix probability coefficient.')\n","flags.DEFINE_integer('augmix_depth', -1,\n","                     'Augmix depth, -1 meaning sampled depth. This corresponds'\n","                     'to line 7 in the Algorithm box in [4].')\n","flags.DEFINE_integer('augmix_width', 3,\n","                     'Augmix width. This corresponds to the k in line 5 in the'\n","                     'Algorithm box in [4].')\n","\n","# Fine-grained specification of the hyperparameters (used when FLAGS.l2 is None)\n","flags.DEFINE_float('bn_l2', None, 'L2 reg. coefficient for batch-norm layers.')\n","flags.DEFINE_float('input_conv_l2', None,\n","                   'L2 reg. coefficient for the input conv layer.')\n","flags.DEFINE_float('group_1_conv_l2', None,\n","                   'L2 reg. coefficient for the 1st group of conv layers.')\n","flags.DEFINE_float('group_2_conv_l2', None,\n","                   'L2 reg. coefficient for the 2nd group of conv layers.')\n","flags.DEFINE_float('group_3_conv_l2', None,\n","                   'L2 reg. coefficient for the 3rd group of conv layers.')\n","flags.DEFINE_float('dense_kernel_l2', None,\n","                   'L2 reg. coefficient for the kernel of the dense layer.')\n","flags.DEFINE_float('dense_bias_l2', None,\n","                   'L2 reg. coefficient for the bias of the dense layer.')\n","\n","\n","flags.DEFINE_bool('collect_profile', False,\n","                  'Whether to trace a profile with tensorboard')\n","\n","FLAGS = flags.FLAGS\n","\n","\n","# def _extract_hyperparameter_dictionary():\n","#   \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","#   flags_as_dict = FLAGS.flag_values_dict()\n","#   hp_keys = ub.models.models.wide_resnet.HP_KEYS\n","#   hps = {k: flags_as_dict[k] for k in hp_keys}\n","#   return hps\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps  \n","\n","\n","def main(argv):\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  formatter = logging.PythonFormatter(fmt)\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(FLAGS.output_dir)\n","  logging.info('Saving checkpoints at %s', FLAGS.output_dir)\n","  tf.random.set_seed(FLAGS.seed)\n","\n","  data_dir = utils.get_data_dir_from_flags(FLAGS)\n","  if FLAGS.use_gpu:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","    # strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\"\n","    #                                                     ,\"/gpu:3\",\"/gpu:4\",\"/gpu:5\",\"/gpu:6\",\"/gpu:7\"\n","    #                                                    ],\n","    #                  cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n","    # strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\"\n","    #                                                      ,\"/gpu:3\",\"/gpu:4\",\"/gpu:5\",\"/gpu:6\",\"/gpu:7\"\n","    #                                                     ])\n","  else:\n","    logging.info('Use TPU at %s',\n","                 FLAGS.tpu if FLAGS.tpu is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder(FLAGS.dataset).info\n","  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * FLAGS.train_proportion)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', FLAGS.train_proportion)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': FLAGS.augmix,\n","      'aug_count': FLAGS.aug_count,\n","      'augmix_depth': FLAGS.augmix_depth,\n","      'augmix_prob_coeff': FLAGS.augmix_prob_coeff,\n","      'augmix_width': FLAGS.augmix_width,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","\n","  seeds = tf.random.experimental.stateless_split(\n","      [FLAGS.seed, FLAGS.seed + 1], 2)[:, 0]\n","  train_builder = get(\n","      FLAGS.dataset,\n","      data_dir=data_dir,\n","      download_data=FLAGS.download_data,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - FLAGS.train_proportion,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if FLAGS.train_proportion < 1.0:\n","    validation_builder = get(\n","        FLAGS.dataset,\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - FLAGS.train_proportion,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = get(\n","      FLAGS.dataset,\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if FLAGS.dataset == 'cifar100' else 10\n","  if FLAGS.corruptions_interval > 0:\n","    if FLAGS.dataset == 'cifar100':\n","      data_dir = FLAGS.cifar100_c_path\n","    corruption_types, _ = utils.load_corrupted_test_info(FLAGS.dataset)\n","    for corruption_type in corruption_types:\n","      for severity in range(1, 6):\n","        dataset = get(\n","            f'{FLAGS.dataset}_corrupted',\n","            corruption_type=corruption_type,\n","            severity=severity,\n","            split=tfds.Split.TEST,\n","            data_dir=data_dir).load(batch_size=batch_size)\n","        test_datasets[f'{corruption_type}_{severity}'] = (\n","            strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(FLAGS.output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","    model = wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=FLAGS.l2,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = FLAGS.base_learning_rate * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n","                       for start_epoch_str in FLAGS.lr_decay_epochs]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=FLAGS.lr_decay_ratio,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=FLAGS.lr_warmup_epochs)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - FLAGS.one_minus_momentum,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=FLAGS.num_bins),\n","      })\n","    if FLAGS.corruptions_interval > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if FLAGS.augmix and FLAGS.aug_count >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if FLAGS.label_smoothing == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing=FLAGS.label_smoothing))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if FLAGS.collect_profile:\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(FLAGS.output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, FLAGS.train_epochs):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * FLAGS.train_epochs\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   FLAGS.train_epochs,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (FLAGS.corruptions_interval > 0 and\n","        (epoch + 1) % FLAGS.corruptions_interval == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (FLAGS.corruptions_interval > 0 and\n","        (epoch + 1) % FLAGS.corruptions_interval == 0):\n","      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (FLAGS.checkpoint_interval > 0 and\n","        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(FLAGS.output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(FLAGS.output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': FLAGS.base_learning_rate,\n","        'one_minus_momentum': FLAGS.one_minus_momentum,\n","        'l2': FLAGS.l2,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I0613 10:02:40.616367 140426208753536 <ipython-input-13-f5ef48ab8eac>:103] [<ipython-input-13-f5ef48ab8eac>:103] Saving checkpoints at /content/gdrive/My Drive/tmp/model\n","I0613 10:02:40.620897 140426208753536 <ipython-input-13-f5ef48ab8eac>:108] [<ipython-input-13-f5ef48ab8eac>:108] Use GPU\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"],"name":"stdout"},{"output_type":"stream","text":["W0613 10:02:40.622979 140426208753536 cross_device_ops.py:1387] [cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"],"name":"stdout"},{"output_type":"stream","text":["I0613 10:02:40.637028 140426208753536 mirrored_strategy.py:369] [mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n","I0613 10:02:40.646151 140426208753536 dataset_info.py:361] [dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n","I0613 10:02:40.651912 140426208753536 <ipython-input-13-f5ef48ab8eac>:130] [<ipython-input-13-f5ef48ab8eac>:130] Steps per epoch 97\n","I0613 10:02:40.653203 140426208753536 <ipython-input-13-f5ef48ab8eac>:131] [<ipython-input-13-f5ef48ab8eac>:131] Size of the dataset 50000\n","I0613 10:02:40.654541 140426208753536 <ipython-input-13-f5ef48ab8eac>:132] [<ipython-input-13-f5ef48ab8eac>:132] Train proportion 1.0\n","I0613 10:02:40.660977 140426208753536 <ipython-input-9-5350e1090691>:121] [<ipython-input-9-5350e1090691>:121] Building dataset cifar10 with additional kwargs:\n","{\n","  \"aug_params\": {\n","    \"aug_count\": 1,\n","    \"augmix\": false,\n","    \"augmix_depth\": -1,\n","    \"augmix_prob_coeff\": 0.5,\n","    \"augmix_width\": 3\n","  },\n","  \"data_dir\": null,\n","  \"download_data\": false,\n","  \"seed\": 1769886085,\n","  \"validation_percent\": 0.0\n","}\n","I0613 10:02:40.663754 140426208753536 dataset_info.py:361] [dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n","I0613 10:02:40.674819 140426208753536 dataset_builder.py:511] [dataset_builder.py:511] Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/cifar10/3.0.2\n","I0613 10:02:41.907674 140426208753536 <ipython-input-9-5350e1090691>:121] [<ipython-input-9-5350e1090691>:121] Building dataset cifar10 with additional kwargs:\n","{\n","  \"data_dir\": null\n","}\n","I0613 10:02:41.909755 140426208753536 dataset_info.py:361] [dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n","I0613 10:02:41.920083 140426208753536 dataset_builder.py:511] [dataset_builder.py:511] Constructing tf.data.Dataset for split test, from /root/tensorflow_datasets/cifar10/3.0.2\n","I0613 10:02:42.775896 140426208753536 <ipython-input-13-f5ef48ab8eac>:201] [<ipython-input-13-f5ef48ab8eac>:201] Building ResNet model\n","I0613 10:02:44.683415 140426208753536 <ipython-input-13-f5ef48ab8eac>:210] [<ipython-input-13-f5ef48ab8eac>:210] Model input shape: (None, 32, 32, 3)\n","I0613 10:02:44.686005 140426208753536 <ipython-input-13-f5ef48ab8eac>:211] [<ipython-input-13-f5ef48ab8eac>:211] Model output shape: (None, 10)\n","I0613 10:02:44.694171 140426208753536 <ipython-input-13-f5ef48ab8eac>:212] [<ipython-input-13-f5ef48ab8eac>:212] Model number of weights: 36497146\n","I0613 10:02:44.887897 140426208753536 <ipython-input-13-f5ef48ab8eac>:356] [<ipython-input-13-f5ef48ab8eac>:356] Starting to run epoch: 0\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rBfCKoPNxix9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4T6xaf_Y5s_Y"},"source":["import tensorflow as tf\n","\n","mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\",\"/gpu:4\",\"/gpu:5\",\"/gpu:6\",\"/gpu:7\"],\n","                      cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNmG5GKz-80A"},"source":["# gs://'colab-sample-bucket-'/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M68k_LJNKeqS"},"source":["project_id = 'lively-fold-315911'\n","\n","import uuid\n","bucket_name = 'good' + str(uuid.uuid1())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5ByEmhP__nz"},"source":["from google.colab import auth\n","auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKibWJjOFjJx","executionInfo":{"status":"ok","timestamp":1623502534073,"user_tz":-540,"elapsed":2011,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"50394d42-d323-4567-cd45-662b4f487798"},"source":["!gsutil iam get gs://bucket_name"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AccessDeniedException: 403 reafs94@gmail.com does not have storage.buckets.getIamPolicy access to the Google Cloud Storage bucket.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciPsR-3AFjR1","executionInfo":{"status":"ok","timestamp":1623502541867,"user_tz":-540,"elapsed":1670,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"3590e45e-363b-47ab-9731-0038a7ecf8ce"},"source":["!gsutil ls -L -b gs://bucket_name"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AccessDeniedException: 403 reafs94@gmail.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iXJPMILeq-rS"},"source":["!gcloud config set project {project_id}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOkeAsz7m1ni","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623499968491,"user_tz":-540,"elapsed":345,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"d95ddc0d-9775-42f7-d5cf-914f3c907de4"},"source":["with open('/tmp/to_upload.txt', 'w') as f:\n","  f.write('my sample file')\n","\n","print('/tmp/to_upload.txt contains:')\n","!cat /tmp/to_upload.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/tmp/to_upload.txt contains:\n","my sample file"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CDdZu7SlOIA","executionInfo":{"status":"ok","timestamp":1623499971954,"user_tz":-540,"elapsed":2177,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"1a3c057d-bfb9-48d2-8286-b09629ff9143"},"source":["!gsutil mb gs://{bucket_name}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating gs://good7d949d48-cb77-11eb-bb93-0242ac1c0002/...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RjflSLzHmX9w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623499974703,"user_tz":-540,"elapsed":1842,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"6bf3135f-0197-4074-a048-72c9f673725a"},"source":["!gsutil cp /tmp/to_upload.txt gs://{bucket_name}/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Copying file:///tmp/to_upload.txt [Content-Type=text/plain]...\n","/ [0 files][    0.0 B/   14.0 B]                                                \r/ [1 files][   14.0 B/   14.0 B]                                                \r\n","Operation completed over 1 objects/14.0 B.                                       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DiKbYEv0iiye","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623499976959,"user_tz":-540,"elapsed":1576,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"4c3fad41-ab28-4bfc-963f-37d382cc07f7"},"source":["!gsutil cat gs://{bucket_name}/to_upload.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["my sample file"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zgXtLY0ScWOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623499981489,"user_tz":-540,"elapsed":2036,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"cead1ffd-c25f-4bcc-de59-3e97a6c4f052"},"source":["!gsutil cp gs://{bucket_name}/to_upload.txt /tmp/gsutil_download.txt\n","  \n","# 전송이 제대로 작동하는지 확인하기 위해 결과를 출력합니다.\n","!cat /tmp/gsutil_download.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Copying gs://good7d949d48-cb77-11eb-bb93-0242ac1c0002/to_upload.txt...\n","/ [0 files][    0.0 B/   14.0 B]                                                \r/ [1 files][   14.0 B/   14.0 B]                                                \r\n","Operation completed over 1 objects/14.0 B.                                       \n","my sample file"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TNECPWqdiGsz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7Gu1y6wiG3G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3sMbNA6iHCo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkFA3MGrb6EM"},"source":["def del_all_flags(FLAGS):\n","    flags_dict = FLAGS._flags()\n","    keys_list = [keys for keys in flags_dict]\n","    \n","    for keys in keys_list:\n","    \tFLAGS.__delattr__(keys)\n","        \n","del_all_flags(flags.FLAGS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOC6J8OaNyKr","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"error","timestamp":1622897960103,"user_tz":-540,"elapsed":1141,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"3436d584-9c74-47c5-9ec0-2035cfbc3109"},"source":["import os\n","import time\n","from absl import app\n","#from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps\n","\n","\n","def main(argv):\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  formatter = logging.PythonFormatter(fmt)\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(output_dir)\n","  logging.info('Saving checkpoints at %s', output_dir)\n","  tf.random.set_seed(2021)\n","  data_dir = None\n","  if False:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","  else:\n","    logging.info('Use TPU at %s',\n","                 None if None is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=None)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder('cifar10').info\n","  batch_size = 64 * 8\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * 1.0)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', 1.0)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': False,\n","      'aug_count': 1,\n","      'augmix_depth': -1,\n","      'augmix_prob_coeff': 0.5,\n","      'augmix_width': 3,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","  seeds = tf.random.experimental.stateless_split(\n","      [2021, 2021 + 1], 2)[:, 0]\n","  train_builder = ub.datasets.get(\n","      'cifar10',\n","      data_dir=data_dir,\n","      download_data=False,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - 1.0,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if 1.0 < 1.0:\n","    validation_builder = ub.datasets.get(\n","        'cifar10',\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - 1.0,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = ub.datasets.get(\n","      'cifar10',\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if 'cifar10' == 'cifar100' else 10\n","  # if -1 > 0:\n","  #   if 'cifar10' == 'cifar100':\n","  #     #data_dir = FLAGS.cifar100_c_path\n","  #   corruption_types, _ = utils.load_corrupted_test_info('cifar10')\n","  #   for corruption_type in corruption_types:\n","  #     for severity in range(1, 6):\n","  #       dataset = ub.datasets.get(\n","  #           'cifar10_corrupted',\n","  #           corruption_type=corruption_type,\n","  #           severity=severity,\n","  #           split=tfds.Split.TEST,\n","  #           data_dir=data_dir).load(batch_size=batch_size)\n","  #       test_datasets[f'{corruption_type}_{severity}'] = (\n","  #           strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","    model = ub.models.wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=2e-4,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = 0.1 * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * 200) // 200\n","                       for start_epoch_str in ['60', '120', '160'] ]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=0.2,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=1)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - 0.1,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=15),\n","      })\n","    if -1 > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=15))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if False and 1 >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if 0 == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing= 0 ))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if False :\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, 200):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * 200\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   200,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (25 > 0 and\n","        (epoch + 1) % 25 == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': 0.1,\n","        'one_minus_momentum': 0.1,\n","        'l2': 2e-4,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FATAL Flags parsing error: Unknown command line flag 'f'\n","Pass --helpshort or --helpfull to see help on flags.\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wsLSzzXCLli0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcV54P2ZRF_v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0q8Mq_PRGJh"},"source":["import os\n","import time\n","from absl import app\n","#from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","#import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","output_dir = '/happy'\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps\n","\n","\n","def main(argv):\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  print('1')\n","  formatter = logging.PythonFormatter(fmt)\n","  print('2')\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(output_dir)\n","  logging.info('Saving checkpoints at %s', output_dir)\n","  tf.random.set_seed(2021)\n","  data_dir = None\n","  if False:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","  else:\n","    logging.info('Use TPU at %s',\n","                 None if None is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=None)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder('cifar10').info\n","  batch_size = 64 * 8\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * 1.0)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', 1.0)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': False,\n","      'aug_count': 1,\n","      'augmix_depth': -1,\n","      'augmix_prob_coeff': 0.5,\n","      'augmix_width': 3,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","  seeds = tf.random.experimental.stateless_split(\n","      [2021, 2021 + 1], 2)[:, 0]\n","  train_builder = ub.datasets.get(\n","      'cifar10',\n","      data_dir=data_dir,\n","      download_data=False,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - 1.0,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if 1.0 < 1.0:\n","    validation_builder = ub.datasets.get(\n","        'cifar10',\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - 1.0,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = ub.datasets.get(\n","      'cifar10',\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if 'cifar10' == 'cifar100' else 10\n","  # if -1 > 0:\n","  #   if 'cifar10' == 'cifar100':\n","  #     #data_dir = FLAGS.cifar100_c_path\n","  #   corruption_types, _ = utils.load_corrupted_test_info('cifar10')\n","  #   for corruption_type in corruption_types:\n","  #     for severity in range(1, 6):\n","  #       dataset = ub.datasets.get(\n","  #           'cifar10_corrupted',\n","  #           corruption_type=corruption_type,\n","  #           severity=severity,\n","  #           split=tfds.Split.TEST,\n","  #           data_dir=data_dir).load(batch_size=batch_size)\n","  #       test_datasets[f'{corruption_type}_{severity}'] = (\n","  #           strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","    model = ub.models.wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=2e-4,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = 0.1 * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * 200) // 200\n","                       for start_epoch_str in ['60', '120', '160'] ]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=0.2,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=1)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - 0.1,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=15),\n","      })\n","    if -1 > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=15))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if False and 1 >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if 0 == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing= 0 ))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if False :\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, 200):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * 200\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   200,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      corrupt_results = aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (25 > 0 and\n","        (epoch + 1) % 25 == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': 0.1,\n","        'one_minus_momentum': 0.1,\n","        'l2': 2e-4,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":null,"outputs":[]}]}