{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd9dbc1",
   "metadata": {},
   "source": [
    "# WideResNet28-10 on CIFAR-10\n",
    "\n",
    "* https://github.com/google/uncertainty-baselines/blob/master/baselines/cifar/deterministic.py#L145\n",
    "* WideResNet28-10 on CIFAR-10 모델 학습 + 체크 포인트 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67e77a",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 flags 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import robustness_metrics as rm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import uncertainty_baselines as ub\n",
    "import utils  # local file import\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "flags.DEFINE_float('label_smoothing', 0., 'Label smoothing parameter in [0,1].')\n",
    "flags.register_validator('label_smoothing',\n",
    "                         lambda ls: ls >= 0.0 and ls <= 1.0,\n",
    "                         message='--label_smoothing must be in [0, 1].')\n",
    "\n",
    "# Data Augmentation flags.\n",
    "flags.DEFINE_bool('augmix', False,\n",
    "                  'Whether to perform AugMix [4] on the input data.')\n",
    "flags.DEFINE_integer('aug_count', 1,\n",
    "                     'Number of augmentation operations in AugMix to perform '\n",
    "                     'on the input image. In the simgle model context, it'\n",
    "                     'should be 1. In the ensembles context, it should be'\n",
    "                     'ensemble_size if we perform random_augment only; It'\n",
    "                     'should be (ensemble_size - 1) if we perform augmix.')\n",
    "flags.DEFINE_float('augmix_prob_coeff', 0.5, 'Augmix probability coefficient.')\n",
    "flags.DEFINE_integer('augmix_depth', -1,\n",
    "                     'Augmix depth, -1 meaning sampled depth. This corresponds'\n",
    "                     'to line 7 in the Algorithm box in [4].')\n",
    "flags.DEFINE_integer('augmix_width', 3,\n",
    "                     'Augmix width. This corresponds to the k in line 5 in the'\n",
    "                     'Algorithm box in [4].')\n",
    "\n",
    "# Fine-grained specification of the hyperparameters (used when FLAGS.l2 is None)\n",
    "flags.DEFINE_float('bn_l2', None, 'L2 reg. coefficient for batch-norm layers.')\n",
    "flags.DEFINE_float('input_conv_l2', None,\n",
    "                   'L2 reg. coefficient for the input conv layer.')\n",
    "flags.DEFINE_float('group_1_conv_l2', None,\n",
    "                   'L2 reg. coefficient for the 1st group of conv layers.')\n",
    "flags.DEFINE_float('group_2_conv_l2', None,\n",
    "                   'L2 reg. coefficient for the 2nd group of conv layers.')\n",
    "flags.DEFINE_float('group_3_conv_l2', None,\n",
    "                   'L2 reg. coefficient for the 3rd group of conv layers.')\n",
    "flags.DEFINE_float('dense_kernel_l2', None,\n",
    "                   'L2 reg. coefficient for the kernel of the dense layer.')\n",
    "flags.DEFINE_float('dense_bias_l2', None,\n",
    "                   'L2 reg. coefficient for the bias of the dense layer.')\n",
    "\n",
    "\n",
    "flags.DEFINE_bool('collect_profile', False,\n",
    "                  'Whether to trace a profile with tensorboard')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f6045",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_hyperparameter_dictionary():\n",
    "  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n",
    "  flags_as_dict = FLAGS.flag_values_dict()\n",
    "  hp_keys = ub.models.models.wide_resnet.HP_KEYS\n",
    "  hps = {k: flags_as_dict[k] for k in hp_keys}\n",
    "  return hps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88846843",
   "metadata": {},
   "source": [
    "## main 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5113269",
   "metadata": {},
   "source": [
    "### log 값들을 처리하는 로깅 처리기 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n",
    "  formatter = logging.PythonFormatter(fmt)\n",
    "  logging.get_absl_handler().setFormatter(formatter)\n",
    "  del argv  # argv 는 사용 안하므로 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d963e",
   "metadata": {},
   "source": [
    "### output 디렉토리,  global seed 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  tf.io.gfile.makedirs(FLAGS.output_dir) # 디렉토리 작성(상위 or 중간)\n",
    "  logging.info('Saving checkpoints at %s', FLAGS.output_dir) # 로그 정보를 알려줌(출력X)\n",
    "  tf.random.set_seed(FLAGS.seed) # global한 random seed 생성\n",
    "\n",
    "  data_dir = FLAGS.data_dir # data가 저장되어있는 data_dir 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800f286",
   "metadata": {},
   "source": [
    "### GPU or TPU 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acddfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  if FLAGS.use_gpu: #GPU 환경 설정\n",
    "    logging.info('Use GPU')\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "  else: # TPU 환경 설정\n",
    "    logging.info('Use TPU at %s',\n",
    "                 FLAGS.tpu if FLAGS.tpu is not None else 'local')\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ds_info = tfds.builder(FLAGS.dataset).info # tfds.core.DatasetBuilder의 dataset(=cifar10) 가져와서 정보 저장\n",
    "  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores # batch_size 설정, 64*8\n",
    "  train_dataset_size = ( # train_dataset_size 선언, 50000 * 1.0\n",
    "      ds_info.splits['train'].num_examples * FLAGS.train_proportion)\n",
    "  steps_per_epoch = int(train_dataset_size / batch_size) # steps_per_epoch 선언, 50000/(64*8)\n",
    "  logging.info('Steps per epoch %s', steps_per_epoch)# 로그 정보를 알려줌(출력X)\n",
    "  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)# 로그 정보를 알려줌(출력X)\n",
    "  logging.info('Train proportion %s', FLAGS.train_proportion)# 로그 정보를 알려줌(출력X)\n",
    "  steps_per_eval = ds_info.splits['test'].num_examples // batch_size #Steps_per_epoch 선언, 10000//(64*8)\n",
    "  num_classes = ds_info.features['label'].num_classes #num_classes 선언, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e34905",
   "metadata": {},
   "source": [
    "### aug_params 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5561b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  aug_params = {\n",
    "      'augmix': FLAGS.augmix, #입력 데이터에 augmix를 수행할지 여부, False\n",
    "      'aug_count': FLAGS.aug_count, #augmix에서 수행할 증강 작업 수, 1\n",
    "      'augmix_depth': FLAGS.augmix_depth, #augmix 확률계수, 0.5\n",
    "      'augmix_prob_coeff': FLAGS.augmix_prob_coeff, #표본의 깊이, -1\n",
    "      'augmix_width': FLAGS.augmix_width, # augmix 폭, 3\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a20f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "  seeds = tf.random.experimental.stateless_split( #2개의 seed 생성, [1769886085, 86449935]\n",
    "      [FLAGS.seed, FLAGS.seed + 1], 2)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85015f4",
   "metadata": {},
   "source": [
    "### 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  train_builder = ub.datasets.get( #train_dataset 생성\n",
    "      FLAGS.dataset,\n",
    "      data_dir=data_dir,\n",
    "      download_data=FLAGS.download_data,\n",
    "      split=tfds.Split.TRAIN,\n",
    "      seed=seeds[0],\n",
    "      aug_params=aug_params,\n",
    "      validation_percent=1. - FLAGS.train_proportion,)\n",
    "  train_dataset = train_builder.load(batch_size=batch_size) # 생성된 train 데이터로 train_dataset 선언\n",
    "  validation_dataset = None # validation_dataset 선언, None, (trainproportion이 1.0이다)\n",
    "  steps_per_validation = 0 # steps_per_validation 선언, 0\n",
    "  clean_test_builder = ub.datasets.get( #test_dataset 생성\n",
    "      FLAGS.dataset,\n",
    "      split=tfds.Split.TEST,\n",
    "      data_dir=data_dir)\n",
    "  clean_test_dataset = clean_test_builder.load(batch_size=batch_size) # 생성된 test 데이터로 clean_test_dataset 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b83b6",
   "metadata": {},
   "source": [
    "### 분산환경에 맞춰 다시 데이터셋 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "  test_datasets = {\n",
    "      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n",
    "  }\n",
    "  steps_per_epoch = train_builder.num_examples // batch_size #steps_per_epoch 선언\n",
    "  steps_per_eval = clean_test_builder.num_examples // batch_size #steps_per_eval 선언\n",
    "  num_classes = 100 if FLAGS.dataset == 'cifar100' else 10 #num_classes 선언, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30c33f",
   "metadata": {},
   "source": [
    "### 지정된 디렉토리에 저장될 요약 파일 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fe7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  summary_writer = tf.summary.create_file_writer( #지정된 디렉토리에 저장될 요약 파일 작성\n",
    "      os.path.join(FLAGS.output_dir, 'summaries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3e2af",
   "metadata": {},
   "source": [
    "### 여러 장치로 분산시켜 훈련(TPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  with strategy.scope(): #환경설정된 gpu or tpu 오픈\n",
    "    logging.info('Building ResNet model') # 로그 정보를 알려줌(출력X)\n",
    "    model = ub.models.wide_resnet( #wide_resnet28-10 모델 구축\n",
    "        input_shape=(32, 32, 3),\n",
    "        depth=28,\n",
    "        width_multiplier=10,\n",
    "        num_classes=num_classes,\n",
    "        l2=FLAGS.l2,\n",
    "        hps=_extract_hyperparameter_dictionary(),\n",
    "        seed=seeds[1])\n",
    "    logging.info('Model input shape: %s', model.input_shape) # 로그 정보를 알려줌(출력X)\n",
    "    logging.info('Model output shape: %s', model.output_shape) # 로그 정보를 알려줌(출력X)\n",
    "    logging.info('Model number of weights: %s', model.count_params()) # 로그 정보를 알려줌(출력X)\n",
    "    base_lr = FLAGS.base_learning_rate * batch_size / 128 #learningRate 선언\n",
    "    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200 #decay_epochs 선언, (60,120,180)*(200//200)\n",
    "                       for start_epoch_str in FLAGS.lr_decay_epochs]\n",
    "    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule( #학습속도 조정(=tf.keras.optimizers.schedules.LearningRateScheduler)\n",
    "        steps_per_epoch,\n",
    "        base_lr,\n",
    "        decay_ratio=FLAGS.lr_decay_ratio,\n",
    "        decay_epochs=lr_decay_epochs,\n",
    "        warmup_epochs=FLAGS.lr_warmup_epochs)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr_schedule, #Gradient descent optimizer 선언\n",
    "                                        momentum=1.0 - FLAGS.one_minus_momentum,\n",
    "                                        nesterov=True)\n",
    "    metrics = { #평가지표 행렬 선언\n",
    "        'train/negative_log_likelihood': #train 데이터셋의 NLL\n",
    "            tf.keras.metrics.Mean(),\n",
    "        'train/accuracy': #train 데이터셋의 accuracy\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        'train/loss': #train 데이터셋의 loss\n",
    "            tf.keras.metrics.Mean(),\n",
    "        'train/ece': #train 데이터셋의 ece\n",
    "            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n",
    "        'test/negative_log_likelihood': #test 데이터셋의 NLL\n",
    "            tf.keras.metrics.Mean(),\n",
    "        'test/accuracy': #test 데이터셋의 accuracy\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        'test/ece': #test 데이터셋의 ece\n",
    "            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n",
    "    }\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer) #checkpoint에 모델과 optimizer 읽을 수 있는 값을 저장\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir) # 최근에 저장된 checkpoint 파일의 파일 이름 저장, 없으면 false반환\n",
    "    initial_epoch = 0 #초기 epoch 선언, 모델을 돌렸을 때 중간에 멈췄다면 이어서 돌릴 수 있음\n",
    "    if latest_checkpoint: #전에 학습한 체크포인트가 있으면\n",
    "      checkpoint.restore(latest_checkpoint) #체크포인트를 latest_checkpoint로 update\n",
    "      logging.info('Loaded checkpoint %s', latest_checkpoint) #로그 정보를 알려줌(출력X)\n",
    "      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch #initial_epoch을 latest_checkpoint 이후로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a814b",
   "metadata": {},
   "source": [
    "### Train\n",
    "* loss에 대하여 layer의 학습가능한 weight의 gradient 를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  @tf.function\n",
    "  def train_step(iterator):\n",
    "    \"\"\"Training StepFn.\"\"\"\n",
    "    def step_fn(inputs):\n",
    "      \"\"\"Per-Replica StepFn.\"\"\"\n",
    "      images = inputs['features'] #변수 선언\n",
    "      labels = inputs['labels'] #라벨 선언\n",
    "\n",
    "      with tf.GradientTape() as tape: #레이어가 입력에 적용하는 연산은 gradient Tape에 기록, 자동적으로 미분\n",
    "        logits = model(images, training=True) # logits 저장\n",
    "        if FLAGS.label_smoothing == 0.:\n",
    "          negative_log_likelihood = tf.reduce_mean( #sparse_categorical_crossentropy의 모든 텐서 차원의 평균 계산\n",
    "              tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
    "                                                              logits,\n",
    "                                                              from_logits=True))\n",
    "        \n",
    "        l2_loss = sum(model.losses) # l2 로스 계산\n",
    "        loss = negative_log_likelihood + l2_loss # loss 계산\n",
    "        scaled_loss = loss / strategy.num_replicas_in_sync #loss 스케일링(장치 수로 나누기(=8))\n",
    "\n",
    "      grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables)) #optimizer 를 통해 업데이트 되는 gradient를 사용\n",
    "\n",
    "      probs = tf.nn.softmax(logits) #softmax activation\n",
    "      metrics['train/ece'].add_batch(probs, label=labels) #train/ece update\n",
    "      metrics['train/loss'].update_state(loss) #train/loss update\n",
    "      metrics['train/negative_log_likelihood'].update_state( #train/negative_log_likelihood update\n",
    "          negative_log_likelihood)\n",
    "      metrics['train/accuracy'].update_state(labels, logits) #train/accuracy update\n",
    "\n",
    "    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)): #올바른 복제본 별 데이터를 단위에 맞춰서 제공\n",
    "      strategy.run(step_fn, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e048407",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ac705",
   "metadata": {},
   "outputs": [],
   "source": [
    "  @tf.function\n",
    "  def test_step(iterator, dataset_split, dataset_name, num_steps):\n",
    "    \"\"\"Evaluation StepFn.\"\"\"\n",
    "    def step_fn(inputs):\n",
    "      \"\"\"Per-Replica StepFn.\"\"\"\n",
    "      images = inputs['features'] #변수 선언\n",
    "      labels = inputs['labels'] #라벨 선언\n",
    "      logits = model(images, training=False) #logits 저장\n",
    "      probs = tf.nn.softmax(logits) #softmax activation\n",
    "      negative_log_likelihood = tf.reduce_mean( # NLL 계산\n",
    "          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n",
    "\n",
    "      if dataset_name == 'clean':\n",
    "        metrics[f'{dataset_split}/negative_log_likelihood'].update_state( #test/negative_log_likelihood update\n",
    "            negative_log_likelihood)\n",
    "        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs) #test/accuracy update\n",
    "        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels) #test/ece update\n",
    "\n",
    "    for _ in tf.range(tf.cast(num_steps, tf.int32)): #올바른 복제본 별 데이터를 단위에 맞춰서 제공\n",
    "      strategy.run(step_fn, args=(next(iterator),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90369da3",
   "metadata": {},
   "source": [
    "### 학습, 검증 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632fd6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  train_iterator = iter(train_dataset)\n",
    "  start_time = time.time() #시작 시간 저장\n",
    "  \n",
    "\n",
    "  for epoch in range(initial_epoch, FLAGS.train_epochs): #학습, 검증 epoch 진행, (0,200)\n",
    "    logging.info('Starting to run epoch: %s', epoch)# 로그 정보를 알려줌(출력X)\n",
    "    train_start_time = time.time()  # 시작 시간 저장\n",
    "    train_step(train_iterator)  #train-step 함수 시작(학습 진행)\n",
    "    \n",
    "    # 시간 계산\n",
    "    current_step = (epoch + 1) * steps_per_epoch\n",
    "    max_steps = steps_per_epoch * FLAGS.train_epochs\n",
    "    time_elapsed = time.time() - start_time\n",
    "    steps_per_sec = float(current_step) / time_elapsed\n",
    "    eta_seconds = (max_steps - current_step) / steps_per_sec\n",
    "    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n",
    "               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n",
    "                   current_step / max_steps,\n",
    "                   epoch + 1,\n",
    "                   FLAGS.train_epochs,\n",
    "                   steps_per_sec,\n",
    "                   eta_seconds / 60,\n",
    "                   time_elapsed / 60))\n",
    "    logging.info(message)# 로그 정보를 알려줌(출력X)\n",
    "    \n",
    "\n",
    "    datasets_to_evaluate = {'clean': test_datasets['clean']}\n",
    "    for dataset_name, test_dataset in datasets_to_evaluate.items():\n",
    "      test_iterator = iter(test_dataset)\n",
    "      logging.info('Testing on dataset %s', dataset_name)# 로그 정보를 알려줌(출력X)\n",
    "      logging.info('Starting to run eval at epoch: %s', epoch)# 로그 정보를 알려줌(출력X)\n",
    "      test_start_time = time.time() #시작 시간 저장\n",
    "      test_step(test_iterator, 'test', dataset_name, steps_per_eval) #test-step 함수 시작(검증 진행)\n",
    "\n",
    "      logging.info('Done with testing on %s', dataset_name)# 로그 정보를 알려줌(출력X)\n",
    "\n",
    "\n",
    "    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',# 로그 정보를 알려줌(출력X)\n",
    "                 metrics['train/loss'].result(),\n",
    "                 metrics['train/accuracy'].result() * 100)\n",
    "    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',# 로그 정보를 알려줌(출력X)\n",
    "                 metrics['test/negative_log_likelihood'].result(),\n",
    "                 metrics['test/accuracy'].result() * 100)\n",
    "    \n",
    "    total_results = {name: metric.result() for name, metric in metrics.items()} #전체 reslut 저장\n",
    "    total_results = { # key value 형태로 저장되어 있는 값들 정렬\n",
    "        k: (list(v.values())[0] if isinstance(v, dict) else v)\n",
    "        for k, v in total_results.items()\n",
    "    }\n",
    "    with summary_writer.as_default(): # summary 로그 기록\n",
    "      for name, result in total_results.items():\n",
    "        tf.summary.scalar(name, result, step=epoch + 1)\n",
    "\n",
    "    for metric in metrics.values(): # metric 초기화\n",
    "      metric.reset_states()\n",
    "\n",
    "    if (FLAGS.checkpoint_interval > 0 and #checkpoint_interval 마다 checkpoint 저장, checkpoint_interval=25\n",
    "        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n",
    "      checkpoint_name = checkpoint.save(\n",
    "          os.path.join(FLAGS.output_dir, 'checkpoint'))\n",
    "      logging.info('Saved checkpoint to %s', checkpoint_name)# 로그 정보를 알려줌(출력X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d0fd3",
   "metadata": {},
   "source": [
    "### final checkpoint, summary 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7313d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "  final_checkpoint_name = checkpoint.save( #fianl checkpoint 저장\n",
    "      os.path.join(FLAGS.output_dir, 'checkpoint'))\n",
    "  logging.info('Saved last checkpoint to %s', final_checkpoint_name) # 로그 정보를 알려줌(출력X)\n",
    "  with summary_writer.as_default(): #summary 로그 기록\n",
    "    hp.hparams({\n",
    "        'base_learning_rate': FLAGS.base_learning_rate, #base_learning_rate, 0.1\n",
    "        'one_minus_momentum': FLAGS.one_minus_momentum, #one_minus_momentum, 0.1\n",
    "        'l2': FLAGS.l2, #l2, 2e-4\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6e0bb",
   "metadata": {},
   "source": [
    "# main 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
